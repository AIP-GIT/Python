{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# M1\tIntroduction and Context\n",
        "1\tML and DL:\n",
        "1.\tPerformance:\n",
        "\n",
        "\n",
        "a.\tMetrics: Time Complexity of Algorithms and Running Time; Memory, Response Time\n"
      ],
      "metadata": {
        "id": "InOh5dILUqc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "\n",
        "# Function to measure the time complexity\n",
        "def measure_time_complexity(data_size):\n",
        "    X, y = datasets.make_classification(n_samples=data_size, n_features=20, random_state=42)\n",
        "    start_time = time.time()\n",
        "    # Your algorithm with time complexity measurement here\n",
        "    elapsed_time = time.time() - start_time\n",
        "    return elapsed_time\n",
        "\n",
        "# Function to measure the running time\n",
        "def measure_running_time(X_train, y_train, X_test, y_test):\n",
        "    start_time = time.time()\n",
        "    # Your algorithm with running time measurement here\n",
        "    model = SVC()\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    return elapsed_time, accuracy\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Generate sample data\n",
        "    X, y = datasets.make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Measure time complexity\n",
        "    data_size = 1000\n",
        "    time_complexity = measure_time_complexity(data_size)\n",
        "    print(f\"Time complexity for {data_size} data points: {time_complexity} seconds\")\n",
        "\n",
        "    # Measure running time\n",
        "    running_time, accuracy = measure_running_time(X_train, y_train, X_test, y_test)\n",
        "    print(f\"Running time: {running_time} seconds\")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "BEdPVbRFVxTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b.\tScaling and Tuning of Performance\n"
      ],
      "metadata": {
        "id": "1Fh2kv4DVdYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Function to perform hyperparameter tuning\n",
        "def perform_grid_search(X_train_scaled, y_train):\n",
        "    param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.01, 0.1, 1, 10]}\n",
        "    svm_model = SVC()\n",
        "    grid_search = GridSearchCV(svm_model, param_grid, cv=3)\n",
        "    start_time = time.time()\n",
        "    grid_search.fit(X_train_scaled, y_train)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    best_params = grid_search.best_params_\n",
        "    return best_params, elapsed_time\n",
        "\n",
        "# Function to scale input features\n",
        "def scale_features(X_train, X_test):\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    return X_train_scaled, X_test_scaled\n",
        "\n",
        "# Function to train a neural network\n",
        "def train_neural_network(X_train_scaled, y_train):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    return model, elapsed_time\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Generate sample data\n",
        "    X, y = datasets.make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Scale features\n",
        "    X_train_scaled, X_test_scaled = scale_features(X_train, X_test)\n",
        "\n",
        "    # Perform hyperparameter tuning\n",
        "    best_params, grid_search_time = perform_grid_search(X_train_scaled, y_train)\n",
        "    print(f\"Best hyperparameters: {best_params}\")\n",
        "    print(f\"Grid search time: {grid_search_time} seconds\")\n",
        "\n",
        "    # Train a neural network\n",
        "    model, training_time = train_neural_network(X_train_scaled, y_train)\n",
        "    print(f\"Neural network training time: {training_time} seconds\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    predictions = model.predict(X_test_scaled)\n",
        "    predictions = (predictions > 0.5).astype(int)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f\"Neural network accuracy: {accuracy}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "i5H1sD00Vygm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\tEnvironments:\n",
        "\n",
        "\n",
        "a.\tTraining vs. Deployment\n"
      ],
      "metadata": {
        "id": "qL9xjuQsVfnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import joblib\n",
        "\n",
        "# Function to train a classifier\n",
        "def train_classifier(X_train, y_train, model_type='random_forest'):\n",
        "    if model_type == 'random_forest':\n",
        "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    elif model_type == 'neural_network':\n",
        "        model = Sequential()\n",
        "        model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model type\")\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "# Function to deploy a trained model\n",
        "def deploy_model(model, X):\n",
        "    if isinstance(model, RandomForestClassifier):\n",
        "        predictions = model.predict(X)\n",
        "    elif isinstance(model, Sequential):\n",
        "        predictions = (model.predict(X) > 0.5).astype(int).flatten()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model type\")\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Generate sample data\n",
        "    X, y = datasets.make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train a random forest classifier\n",
        "    rf_model = train_classifier(X_train, y_train, model_type='random_forest')\n",
        "\n",
        "    # Save the trained model\n",
        "    joblib.dump(rf_model, 'random_forest_model.joblib')\n",
        "\n",
        "    # Deploy the trained model on new data (in a different script or environment)\n",
        "    loaded_rf_model = joblib.load('random_forest_model.joblib')\n",
        "    predictions = deploy_model(loaded_rf_model, X_test)\n",
        "\n",
        "    # Evaluate the deployed model\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f\"Random Forest Model Accuracy: {accuracy}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "gH7BWDKSVzIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b.\tRange of Systems:\n",
        "Distributed and Cloud, Embedded and Mobile.\n",
        "\n"
      ],
      "metadata": {
        "id": "2dPYRCiaVi1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.externals import joblib  # For scikit-learn version <= 0.22\n",
        "# For scikit-learn version >= 0.23, use: from joblib import dump, load\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Function to train a classifier\n",
        "def train_classifier(X_train, y_train, model_type='random_forest'):\n",
        "    if model_type == 'random_forest':\n",
        "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    elif model_type == 'neural_network':\n",
        "        model = Sequential()\n",
        "        model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model type\")\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "# Function to save the trained model\n",
        "def save_model(model, filename):\n",
        "    if isinstance(model, RandomForestClassifier):\n",
        "        joblib.dump(model, filename)\n",
        "    elif isinstance(model, Sequential):\n",
        "        model.save(filename)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model type\")\n",
        "\n",
        "# Distributed/Cloud Environment Training\n",
        "def train_cloud_model():\n",
        "    # Generate sample data\n",
        "    X, y = datasets.make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "    X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train a random forest classifier\n",
        "    rf_model = train_classifier(X_train, y_train, model_type='random_forest')\n",
        "\n",
        "    # Save the trained model\n",
        "    save_model(rf_model, 'cloud_model.joblib')\n",
        "\n",
        "# Mobile/Embedded Deployment\n",
        "def deploy_mobile_model(X_test):\n",
        "    # Load the trained model\n",
        "    loaded_rf_model = joblib.load('cloud_model.joblib')\n",
        "\n",
        "    # Deploy the model on mobile/embedded system\n",
        "    predictions = loaded_rf_model.predict(X_test)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Distributed/Cloud Environment Training\n",
        "    train_cloud_model()\n",
        "\n",
        "    # Mobile/Embedded Deployment\n",
        "    # In a real-world scenario, X_test_mobile would come from your mobile/embedded system\n",
        "    X_test_mobile = np.random.rand(10, 20)  # Random test data for illustration\n",
        "    predictions_mobile = deploy_mobile_model(X_test_mobile)\n",
        "\n",
        "    print(\"Predictions on Mobile/Embedded System:\", predictions_mobile)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "5LDJ5a5AVzlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2\tParallel and Distributed Algorithms:\n",
        "\n",
        "\n",
        "1.\tSystems and Performance;\n"
      ],
      "metadata": {
        "id": "LkChdmR8VlHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Function to perform parallel processing with scikit-learn\n",
        "def perform_parallel_processing(X_train, y_train):\n",
        "    param_grid = {'n_estimators': [10, 50, 100, 200], 'max_depth': [None, 10, 20, 30]}\n",
        "    rf_model = RandomForestClassifier()\n",
        "    grid_search = GridSearchCV(rf_model, param_grid, cv=3, n_jobs=-1)\n",
        "\n",
        "    start_time = time.time()\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    best_params = grid_search.best_params_\n",
        "    return best_params, elapsed_time\n",
        "\n",
        "# Function to perform distributed training with TensorFlow\n",
        "def perform_distributed_training(X_train, y_train, strategy):\n",
        "    with strategy.scope():\n",
        "        model = Sequential()\n",
        "        model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train, epochs=3, batch_size=32, verbose=0)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    return model, elapsed_time\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Generate sample data\n",
        "    X, y = datasets.make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Perform parallel processing with scikit-learn\n",
        "    parallel_params, parallel_time = perform_parallel_processing(X_train, y_train)\n",
        "    print(f\"Best hyperparameters from parallel processing: {parallel_params}\")\n",
        "    print(f\"Parallel processing time: {parallel_time} seconds\")\n",
        "\n",
        "    # Perform distributed training with TensorFlow\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "    print(f'Number of devices: {strategy.num_replicas_in_sync}')\n",
        "\n",
        "    X_train_distributed, y_train_distributed = strategy.experimental_distribute_dataset((X_train, y_train))\n",
        "    model, distributed_time = perform_distributed_training(X_train_distributed, y_train_distributed, strategy)\n",
        "    print(f\"Distributed training time: {distributed_time} seconds\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    predictions = model.predict(X_test)\n",
        "    predictions = (predictions > 0.5).astype(int).flatten()\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f\"Neural network accuracy: {accuracy}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "4WEFPi2gVz-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\tSpeedup – Approaches and Issues;\n"
      ],
      "metadata": {
        "id": "B5eQVZ4rVoYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Parallelization with scikit-learn:"
      ],
      "metadata": {
        "id": "IJ5HsRGyO1Uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from joblib import parallel_backend\n",
        "\n",
        "# Function to train a random forest classifier\n",
        "def train_classifier(X_train, y_train, n_jobs=1):\n",
        "    with parallel_backend('threading', n_jobs=n_jobs):\n",
        "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Generate sample data\n",
        "    X, y = datasets.make_classification(n_samples=10000, n_features=20, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train a random forest classifier with parallelization\n",
        "    start_time = time.time()\n",
        "    rf_model = train_classifier(X_train, y_train, n_jobs=2)\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Evaluate the model\n",
        "    predictions = rf_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "    print(f\"Random Forest Training Time: {elapsed_time} seconds\")\n",
        "    print(f\"Random Forest Accuracy: {accuracy}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "7JCSUy4wO85f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. GPU Acceleration with TensorFlow:\n"
      ],
      "metadata": {
        "id": "m_EzgkrTO2VT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Function to train a neural network\n",
        "def train_neural_network(X_train, y_train, use_gpu=True):\n",
        "    if use_gpu:\n",
        "        physical_devices = tf.config.list_physical_devices('GPU')\n",
        "        if physical_devices:\n",
        "            tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    return model, elapsed_time\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Generate sample data\n",
        "    X, y = datasets.make_classification(n_samples=10000, n_features=20, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train a neural network with GPU acceleration\n",
        "    start_time = time.time()\n",
        "    model, training_time = train_neural_network(X_train, y_train, use_gpu=True)\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Evaluate the model\n",
        "    predictions = (model.predict(X_test) > 0.5).astype(int).flatten()\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "    print(f\"Neural Network Training Time: {elapsed_time} seconds\")\n",
        "    print(f\"Neural Network Accuracy: {accuracy}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "6r2ZlwgWV0U5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.\tData Parallelism vs. Task Parallelism vs. Request Parallelism.\n"
      ],
      "metadata": {
        "id": "oYIou40yVppC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Parallelism with scikit-learn:"
      ],
      "metadata": {
        "id": "-1d6IVmYQDWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# Function to train a random forest classifier on a subset of data\n",
        "def train_classifier_subset(X_subset, y_subset):\n",
        "    model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "    model.fit(X_subset, y_subset)\n",
        "    return model\n",
        "\n",
        "# Function to perform data parallelism\n",
        "def data_parallelism(X_train, y_train, n_jobs=1):\n",
        "    num_samples = X_train.shape[0]\n",
        "    samples_per_job = num_samples // n_jobs\n",
        "\n",
        "    # Split the data into subsets for parallel processing\n",
        "    data_subsets = [(X_train[i:i+samples_per_job], y_train[i:i+samples_per_job])\n",
        "                    for i in range(0, num_samples, samples_per_job)]\n",
        "\n",
        "    # Train the classifier on each subset in parallel\n",
        "    models = Parallel(n_jobs=n_jobs)(delayed(train_classifier_subset)(*subset) for subset in data_subsets)\n",
        "\n",
        "    # Ensemble the models (e.g., average predictions for RandomForest)\n",
        "    return models\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Generate sample data\n",
        "    X, y = datasets.make_classification(n_samples=10000, n_features=20, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Perform data parallelism\n",
        "    start_time = time.time()\n",
        "    models = data_parallelism(X_train, y_train, n_jobs=2)\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Aggregate predictions for ensemble models\n",
        "    predictions = np.mean([model.predict(X_test) for model in models], axis=0)\n",
        "    predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "    # Evaluate the ensemble model\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f\"Ensemble Model Training Time: {elapsed_time} seconds\")\n",
        "    print(f\"Ensemble Model Accuracy: {accuracy}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Mt4MLrA5V0sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task Parallelism with TensorFlow:"
      ],
      "metadata": {
        "id": "pnhK7mIPQH3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Function to train a neural network on a separate task\n",
        "def train_neural_network_task(X_subset, y_subset):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=X_subset.shape[1], activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_subset, y_subset, epochs=3, batch_size=32, verbose=0)\n",
        "    return model\n",
        "\n",
        "# Function to perform task parallelism\n",
        "def task_parallelism(X_train, y_train, num_tasks=2):\n",
        "    num_samples = X_train.shape[0]\n",
        "    samples_per_task = num_samples // num_tasks\n",
        "\n",
        "    # Split the data into subsets for parallel processing\n",
        "    data_subsets = [(X_train[i:i+samples_per_task], y_train[i:i+samples_per_task])\n",
        "                    for i in range(0, num_samples, samples_per_task)]\n",
        "\n",
        "    # Train the neural network on each subset as a separate task\n",
        "    models = [train_neural_network_task(*subset) for subset in data_subsets]\n",
        "\n",
        "    # Combine the models if needed\n",
        "\n",
        "    return models\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Generate sample data\n",
        "    X, y = datasets.make_classification(n_samples=10000, n_features=20, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Perform task parallelism\n",
        "    start_time = time.time()\n",
        "    models = task_parallelism(X_train, y_train, num_tasks=2)\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Aggregate predictions or perform further tasks if needed\n",
        "\n",
        "    print(f\"Task Parallelism Training Time: {elapsed_time} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ryDbqCZUQK0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Request parallelism, often associated with handling multiple independent requests simultaneously, is not explicitly demonstrated in the provided examples as it's more relevant in deployment scenarios like serving models in a web service."
      ],
      "metadata": {
        "id": "DI7KS-06QUmH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.\tScale-out Clusters – Cost of communication and impact on Speedup\n",
        "\n"
      ],
      "metadata": {
        "id": "JcER0P4EVrC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.externals import joblib  # For scikit-learn version <= 0.22\n",
        "\n",
        "# Function to train a random forest classifier on a subset of data\n",
        "def train_classifier_subset(X_subset, y_subset):\n",
        "    model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "    model.fit(X_subset, y_subset)\n",
        "    return model\n",
        "\n",
        "# Function to perform data parallelism\n",
        "def data_parallelism(X_train, y_train, n_jobs=1):\n",
        "    num_samples = X_train.shape[0]\n",
        "    samples_per_job = num_samples // n_jobs\n",
        "\n",
        "    # Split the data into subsets for parallel processing\n",
        "    data_subsets = [(X_train[i:i+samples_per_job], y_train[i:i+samples_per_job])\n",
        "                    for i in range(0, num_samples, samples_per_job)]\n",
        "\n",
        "    # Train the classifier on each subset in parallel\n",
        "    models = joblib.Parallel(n_jobs=n_jobs)(\n",
        "        joblib.delayed(train_classifier_subset)(*subset) for subset in data_subsets\n",
        "    )\n",
        "\n",
        "    # Ensemble the models (e.g., average predictions for RandomForest)\n",
        "    return models\n",
        "\n",
        "# Function to perform distributed training with TensorFlow on a cluster\n",
        "def perform_distributed_training(X_train, y_train, strategy):\n",
        "    with strategy.scope():\n",
        "        model = Sequential()\n",
        "        model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train, epochs=3, batch_size=32, verbose=0)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    return model, elapsed_time\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Generate sample data\n",
        "    X, y = datasets.make_classification(n_samples=10000, n_features=20, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Perform data parallelism with scikit-learn\n",
        "    start_time = time.time()\n",
        "    models = data_parallelism(X_train, y_train, n_jobs=2)\n",
        "    elapsed_time_data_parallelism = time.time() - start_time\n",
        "\n",
        "    # Aggregate predictions for ensemble models\n",
        "    predictions_data_parallelism = np.mean([model.predict(X_test) for model in models], axis=0)\n",
        "    predictions_data_parallelism = (predictions_data_parallelism > 0.5).astype(int)\n",
        "\n",
        "    # Evaluate the ensemble model\n",
        "    accuracy_data_parallelism = accuracy_score(y_test, predictions_data_parallelism)\n",
        "    print(f\"Ensemble Model (Data Parallelism) Training Time: {elapsed_time_data_parallelism} seconds\")\n",
        "    print(f\"Ensemble Model (Data Parallelism) Accuracy: {accuracy_data_parallelism}\")\n",
        "\n",
        "    # Perform distributed training with TensorFlow on a cluster\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "    print(f'Number of devices: {strategy.num_replicas_in_sync}')\n",
        "\n",
        "    X_train_distributed, y_train_distributed = strategy.experimental_distribute_dataset((X_train, y_train))\n",
        "    model, elapsed_time_distributed_training = perform_distributed_training(X_train_distributed, y_train_distributed, strategy)\n",
        "\n",
        "    # Evaluate the model\n",
        "    predictions_distributed_training = (model.predict(X_test) > 0.5).astype(int).flatten()\n",
        "    accuracy_distributed_training = accuracy_score(y_test, predictions_distributed_training)\n",
        "    print(f\"Neural Network Training Time (Distributed): {elapsed_time_distributed_training} seconds\")\n",
        "    print(f\"Neural Network Accuracy (Distributed): {accuracy_distributed_training}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "rUVnuN_jV1D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3\tModern Systems:\n",
        "1.\tParallel Execution on Multicore processors and GPGPUs\n"
      ],
      "metadata": {
        "id": "_mlIbG5qVsTq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parallel Execution on Multicore Processors:"
      ],
      "metadata": {
        "id": "kVH7YmqDQ28s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# Function to train a random forest classifier on a subset of data\n",
        "def train_classifier_subset(X_subset, y_subset):\n",
        "    model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "    model.fit(X_subset, y_subset)\n",
        "    return model\n",
        "\n",
        "# Function to perform parallel execution on multicore processors\n",
        "def parallel_execution(X_train, y_train, n_jobs=1):\n",
        "    num_samples = X_train.shape[0]\n",
        "    samples_per_job = num_samples // n_jobs\n",
        "\n",
        "    # Split the data into subsets for parallel processing\n",
        "    data_subsets = [(X_train[i:i+samples_per_job], y_train[i:i+samples_per_job])\n",
        "                    for i in range(0, num_samples, samples_per_job)]\n",
        "\n",
        "    # Train the classifier on each subset in parallel\n",
        "    models = Parallel(n_jobs=n_jobs)(\n",
        "        delayed(train_classifier_subset)(*subset) for subset in data_subsets\n",
        "    )\n",
        "\n",
        "    # Ensemble the models (e.g., average predictions for RandomForest)\n",
        "    return models\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Generate sample data\n",
        "    X, y = datasets.make_classification(n_samples=10000, n_features=20, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Perform parallel execution on multicore processors\n",
        "    start_time = time.time()\n",
        "    models = parallel_execution(X_train, y_train, n_jobs=2)\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Aggregate predictions for ensemble models\n",
        "    predictions = np.mean([model.predict(X_test) for model in models], axis=0)\n",
        "    predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "    # Evaluate the ensemble model\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f\"Ensemble Model Training Time: {elapsed_time} seconds\")\n",
        "    print(f\"Ensemble Model Accuracy: {accuracy}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "zeyuHT__V1bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPGPU Acceleration with TensorFlow:"
      ],
      "metadata": {
        "id": "kVYreE1-Q8x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Function to train a neural network\n",
        "def train_neural_network(X_train, y_train, use_gpu=True):\n",
        "    if use_gpu:\n",
        "        physical_devices = tf.config.list_physical_devices('GPU')\n",
        "        if physical_devices:\n",
        "            tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    return model, elapsed_time\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Generate sample data\n",
        "    X, y = datasets.make_classification(n_samples=10000, n_features=20, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train a neural network with GPGPU acceleration\n",
        "    start_time = time.time()\n",
        "    model, training_time = train_neural_network(X_train, y_train, use_gpu=True)\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Evaluate the model\n",
        "    predictions = (model.predict(X_test) > 0.5).astype(int).flatten()\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "    print(f\"Neural Network Training Time: {elapsed_time} seconds\")\n",
        "    print(f\"Neural Network Accuracy: {accuracy}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "xWLVLywnRBrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\tDistributed Execution on Clusters:\n",
        "(CPU and GPU clusters) -   Data Distribution Strategies\n"
      ],
      "metadata": {
        "id": "zlBAs8vbVtyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distributed Execution on CPU Cluster:"
      ],
      "metadata": {
        "id": "Ia2UGouFRTf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from joblib import Parallel, delayed\n",
        "import multiprocessing\n",
        "\n",
        "# Function to train a random forest classifier on a subset of data\n",
        "def train_classifier_subset(X_subset, y_subset):\n",
        "    model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "    model.fit(X_subset, y_subset)\n",
        "    return model\n",
        "\n",
        "# Function to perform data parallelism\n",
        "def data_parallelism(X_train, y_train, n_jobs=1):\n",
        "    num_samples = X_train.shape[0]\n",
        "    samples_per_job = num_samples // n_jobs\n",
        "\n",
        "    # Split the data into subsets for parallel processing\n",
        "    data_subsets = [(X_train[i:i+samples_per_job], y_train[i:i+samples_per_job])\n",
        "                    for i in range(0, num_samples, samples_per_job)]\n",
        "\n",
        "    # Train the classifier on each subset in parallel\n",
        "    models = Parallel(n_jobs=n_jobs)(\n",
        "        delayed(train_classifier_subset)(*subset) for subset in data_subsets\n",
        "    )\n",
        "\n",
        "    # Ensemble the models (e.g., average predictions for RandomForest)\n",
        "    return models\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Generate sample data\n",
        "    X, y = datasets.make_classification(n_samples=10000, n_features=20, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define the number of CPU cores available in the cluster\n",
        "    num_cores = multiprocessing.cpu_count()\n",
        "\n",
        "    # Perform data parallelism on the CPU cluster\n",
        "    start_time = time.time()\n",
        "    models = data_parallelism(X_train, y_train, n_jobs=num_cores)\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Aggregate predictions for ensemble models\n",
        "    predictions = np.mean([model.predict(X_test) for model in models], axis=0)\n",
        "    predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "    # Evaluate the ensemble model\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f\"Ensemble Model (Data Parallelism) Training Time: {elapsed_time} seconds\")\n",
        "    print(f\"Ensemble Model (Data Parallelism) Accuracy: {accuracy}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "48uKRL4tRVIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distributed Execution on GPU Cluster with TensorFlow:"
      ],
      "metadata": {
        "id": "KQdQSptDRYOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Function to train a neural network\n",
        "def train_neural_network(X_train, y_train, strategy):\n",
        "    with strategy.scope():\n",
        "        model = Sequential()\n",
        "        model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train, epochs=3, batch_size=32, verbose=0)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    return model, elapsed_time\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Generate sample data\n",
        "    X, y = datasets.make_classification(n_samples=10000, n_features=20, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define the GPU cluster configuration\n",
        "    # Note: Adapt this based on the specific GPU cluster setup\n",
        "    cluster_spec = tf.train.ClusterSpec({\n",
        "        'worker': ['worker1:2222', 'worker2:2222']  # Specify worker nodes and ports\n",
        "    })\n",
        "\n",
        "    # Create a TensorFlow server\n",
        "    server = tf.distribute.Server(cluster_spec, job_name='worker', task_index=0)\n",
        "\n",
        "    # Check if the current process is the chief (master) process\n",
        "    is_chief = (server.task_index == 0)\n",
        "\n",
        "    # Create a MirroredStrategy for distributed training\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "    # Perform distributed training on the GPU cluster\n",
        "    if is_chief:\n",
        "        print(f'Number of devices: {strategy.num_replicas_in_sync}')\n",
        "\n",
        "    X_train_distributed, y_train_distributed = strategy.experimental_distribute_dataset((X_train, y_train))\n",
        "    model, elapsed_time_distributed_training = train_neural_network(X_train_distributed, y_train_distributed, strategy)\n",
        "\n",
        "    # Evaluate the model\n",
        "    predictions_distributed_training = (model.predict(X_test) > 0.5).astype(int).flatten()\n",
        "    accuracy_distributed_training = accuracy_score(y_test, predictions_distributed_training)\n",
        "    print(f\"Neural Network Training Time (Distributed): {elapsed_time_distributed_training} seconds\")\n",
        "    print(f\"Neural Network Accuracy (Distributed): {accuracy_distributed_training}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "IEins1avRcpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# M2 Parallel / Distributed ML algorithms - Overview and Techniques\n",
        "Parallel / Distributed ML algorithms - Overview and Techniques:\n",
        "1.\tCNN\n"
      ],
      "metadata": {
        "id": "Vjff-5fbRsxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import datasets\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Load a sample dataset (e.g., CIFAR-10)\n",
        "(X, y), _ = datasets.cifar10.load_data()\n",
        "X = X.astype(np.float32) / 255.0\n",
        "y = LabelEncoder().fit_transform(y.flatten())\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the CNN model using TensorFlow\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "cf8iY3XVSD1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\tGradient Descent and Stochastic Gradient Descent\n"
      ],
      "metadata": {
        "id": "NbREWMN_SEUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=1, noise=30, random_state=42)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Gradient Descent using scikit-learn\n",
        "gd_model = SGDRegressor(learning_rate='constant', eta0=0.01, max_iter=100)\n",
        "gd_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Stochastic Gradient Descent using scikit-learn\n",
        "sgd_model = SGDRegressor(learning_rate='invscaling', max_iter=100)\n",
        "sgd_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Gradient Descent using TensorFlow\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(1, input_dim=1, activation=None, kernel_initializer='zeros', bias_initializer='zeros'))\n",
        "optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
        "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "model.fit(X_train_scaled, y_train, epochs=100, verbose=0)\n",
        "\n",
        "# Evaluate models\n",
        "gd_predictions = gd_model.predict(X_test_scaled)\n",
        "sgd_predictions = sgd_model.predict(X_test_scaled)\n",
        "tf_predictions = model.predict(X_test_scaled).flatten()\n",
        "\n",
        "# Print the coefficients and performance metrics\n",
        "print(\"Gradient Descent Coefficients:\", gd_model.coef_, gd_model.intercept_)\n",
        "print(\"Stochastic Gradient Descent Coefficients:\", sgd_model.coef_, sgd_model.intercept_)\n",
        "print(\"TensorFlow Gradient Descent Coefficients:\", model.layers[0].get_weights())\n",
        "\n",
        "# Compare predictions\n",
        "print(\"Performance Metrics:\")\n",
        "print(\"Gradient Descent MSE:\", np.mean((y_test - gd_predictions) ** 2))\n",
        "print(\"Stochastic Gradient Descent MSE:\", np.mean((y_test - sgd_predictions) ** 2))\n",
        "print(\"TensorFlow Gradient Descent MSE:\", np.mean((y_test - tf_predictions) ** 2))\n"
      ],
      "metadata": {
        "id": "Kjgw5MJYSetf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.\tSVM\n"
      ],
      "metadata": {
        "id": "tsFm6f8BSwgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# SVM using scikit-learn\n",
        "svm_model = SVC(kernel='linear', C=1)\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "svm_predictions = svm_model.predict(X_test_scaled)\n",
        "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
        "print(\"SVM (scikit-learn) Accuracy:\", svm_accuracy)\n",
        "\n",
        "# SVM using TensorFlow\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(3, input_dim=4, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train_scaled, y_train, epochs=50, verbose=0)\n",
        "\n",
        "# Evaluate TensorFlow SVM\n",
        "tf_predictions = np.argmax(model.predict(X_test_scaled), axis=1)\n",
        "tf_accuracy = accuracy_score(y_test, tf_predictions)\n",
        "print(\"SVM (TensorFlow) Accuracy:\", tf_accuracy)\n"
      ],
      "metadata": {
        "id": "eoSbeabmSyyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.\tk-Means\n"
      ],
      "metadata": {
        "id": "d4ddkj-AS5OX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_blobs(n_samples=300, centers=3, cluster_std=0.70, random_state=0)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# k-Means using scikit-learn\n",
        "kmeans_sklearn = KMeans(n_clusters=3, random_state=0)\n",
        "kmeans_sklearn.fit(X_scaled)\n",
        "labels_sklearn = kmeans_sklearn.predict(X_scaled)\n",
        "\n",
        "# k-Means using TensorFlow\n",
        "model = models.Sequential()\n",
        "model.add(layers.InputLayer(input_shape=(2,)))\n",
        "model.add(layers.Dense(3, activation=None))\n",
        "model.compile(optimizer='adam', loss='kld')\n",
        "model.fit(X_scaled, X_scaled, epochs=100, verbose=0)\n",
        "\n",
        "# Get cluster assignments from TensorFlow model\n",
        "centroids_tf = model.layers[1].get_weights()[0]\n",
        "distances_tf = tf.norm(X_scaled[:, None] - centroids_tf, axis=-1)\n",
        "labels_tf = tf.argmin(distances_tf, axis=-1).numpy()\n",
        "\n",
        "# Print cluster assignments\n",
        "print(\"Cluster Assignments (scikit-learn):\", labels_sklearn)\n",
        "print(\"Cluster Assignments (TensorFlow):\", labels_tf)\n"
      ],
      "metadata": {
        "id": "ExPwfqnDS9zA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.\tkNN\n"
      ],
      "metadata": {
        "id": "vHMImWqVTKd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# kNN using scikit-learn\n",
        "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_model.fit(X_train_scaled, y_train)\n",
        "knn_predictions = knn_model.predict(X_test_scaled)\n",
        "knn_accuracy = accuracy_score(y_test, knn_predictions)\n",
        "print(\"kNN (scikit-learn) Accuracy:\", knn_accuracy)\n",
        "\n",
        "# kNN using TensorFlow\n",
        "model = models.Sequential()\n",
        "model.add(layers.InputLayer(input_shape=(4,)))\n",
        "model.add(layers.Dense(3, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train_scaled, y_train, epochs=50, verbose=0)\n",
        "\n",
        "# Evaluate TensorFlow kNN\n",
        "tf_predictions = np.argmax(model.predict(X_test_scaled), axis=1)\n",
        "tf_accuracy = accuracy_score(y_test, tf_predictions)\n",
        "print(\"kNN (TensorFlow) Accuracy:\", tf_accuracy)\n"
      ],
      "metadata": {
        "id": "rWQziOC7TLpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.\tDecision Trees/Random Forests.\n"
      ],
      "metadata": {
        "id": "F7LXJ1FxTRcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Decision Tree using scikit-learn\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_predictions = dt_model.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
        "print(\"Decision Tree (scikit-learn) Accuracy:\", dt_accuracy)\n",
        "\n",
        "# Random Forest using scikit-learn\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
        "print(\"Random Forest (scikit-learn) Accuracy:\", rf_accuracy)\n",
        "\n",
        "# Decision Tree using TensorFlow\n",
        "model_dt = models.Sequential()\n",
        "model_dt.add(layers.InputLayer(input_shape=(4,)))\n",
        "model_dt.add(layers.Dense(3, activation='softmax'))\n",
        "model_dt.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model_dt.fit(X_train, y_train, epochs=50, verbose=0)\n",
        "\n",
        "# Evaluate TensorFlow Decision Tree\n",
        "tf_dt_predictions = np.argmax(model_dt.predict(X_test), axis=1)\n",
        "tf_dt_accuracy = accuracy_score(y_test, tf_dt_predictions)\n",
        "print(\"Decision Tree (TensorFlow) Accuracy:\", tf_dt_accuracy)\n",
        "\n",
        "# Random Forest using TensorFlow\n",
        "model_rf = models.Sequential()\n",
        "model_rf.add(layers.InputLayer(input_shape=(4,)))\n",
        "model_rf.add(layers.Dense(128, activation='relu'))\n",
        "model_rf.add(layers.Dropout(0.5))\n",
        "model_rf.add(layers.Dense(3, activation='softmax'))\n",
        "model_rf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model_rf.fit(X_train, y_train, epochs=50, verbose=0)\n",
        "\n",
        "# Evaluate TensorFlow Random Forest\n",
        "tf_rf_predictions = np.argmax(model_rf.predict(X_test), axis=1)\n",
        "tf_rf_accuracy = accuracy_score(y_test, tf_rf_predictions)\n",
        "print(\"Random Forest (TensorFlow) Accuracy:\", tf_rf_accuracy)\n"
      ],
      "metadata": {
        "id": "B6bgNeWvTUOx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}