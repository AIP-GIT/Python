{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOqSptg4SPkdAmXBsYfu+gg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**M3**\n","##1.Large Scale Machine Learning Systems\n","##a.\tThe Parameter Server Model\n"],"metadata":{"id":"vXxLY92zQVv6"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.datasets import make_classification\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import tensorflow as tf\n","\n","# Generate synthetic data\n","X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_clusters_per_class=2, random_state=42)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Define the parameter server\n","class ParameterServer:\n","    def __init__(self, model):\n","        self.model = model\n","        self.weights = None\n","\n","    def update_weights(self, new_weights):\n","        self.weights = new_weights\n","\n","    def get_weights(self):\n","        return self.weights\n","\n","# Define the worker\n","class Worker:\n","    def __init__(self, model, parameter_server):\n","        self.model = model\n","        self.parameter_server = parameter_server\n","\n","    def train(self, X, y):\n","        self.model.partial_fit(X, y, classes=np.unique(y))\n","\n","    def update_parameters(self):\n","        weights = self.model.coef_\n","        self.parameter_server.update_weights(weights)\n","\n","# Initialize the parameter server and workers\n","parameter_server = ParameterServer(SGDClassifier(loss='log'))\n","workers = [Worker(SGDClassifier(loss='log'), parameter_server) for _ in range(5)]\n","\n","# Training loop\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    for worker in workers:\n","        # Each worker trains on its own subset of data\n","        worker.train(X_train, y_train)\n","\n","    # Aggregate and update weights on the parameter server\n","    aggregated_weights = np.mean([worker.model.coef_ for worker in workers], axis=0)\n","    parameter_server.update_weights(aggregated_weights)\n","\n","# Evaluate the final model on the test set\n","final_weights = parameter_server.get_weights()\n","y_pred = np.argmax(X_test.dot(final_weights.T), axis=1)\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Final accuracy on test set: {accuracy}\")\n"],"metadata":{"id":"MwF9JqnYRmdO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##b.\tSpark Architecture"],"metadata":{"id":"-tn2lHl4UaFT"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","import tensorflow as tf\n","from sklearn.datasets import make_classification\n","from sklearn.linear_model import SGDClassifier\n","import numpy as np\n","\n","# Create a Spark session\n","spark = SparkSession.builder.appName(\"SparkMLExample\").getOrCreate()\n","\n","# Generate synthetic data\n","X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_clusters_per_class=2, random_state=42)\n","\n","# Create a DataFrame from the numpy arrays\n","data = [(float(y[i]), Vectors.dense(X[i])) for i in range(len(y))]\n","df = spark.createDataFrame(data, [\"label\", \"features\"])\n","\n","# Split the data into training and testing sets\n","train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n","\n","# Define the parameter server\n","class ParameterServer:\n","    def __init__(self, model):\n","        self.model = model\n","        self.weights = None\n","\n","    def update_weights(self, new_weights):\n","        self.weights = new_weights\n","\n","    def get_weights(self):\n","        return self.weights\n","\n","# Define the worker\n","class Worker:\n","    def __init__(self, model, parameter_server):\n","        self.model = model\n","        self.parameter_server = parameter_server\n","\n","    def train(self, data):\n","        self.model.fit(data)\n","\n","    def update_parameters(self):\n","        weights = np.array(self.model.coefficients)\n","        self.parameter_server.update_weights(weights)\n","\n","# Convert Spark DataFrame to TensorFlow Dataset\n","train_data_tf = tf.data.Dataset.from_tensor_slices((np.array(train_data.select(\"features\").collect()), np.array(train_data.select(\"label\").collect())))\n","test_data_tf = tf.data.Dataset.from_tensor_slices((np.array(test_data.select(\"features\").collect()), np.array(test_data.select(\"label\").collect())))\n","\n","# Initialize the parameter server and workers\n","parameter_server = ParameterServer(SGDClassifier(loss='log'))\n","workers = [Worker(LogisticRegression(), parameter_server) for _ in range(5)]\n","\n","# Training loop\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    for worker in workers:\n","        # Each worker trains on its own subset of data\n","        worker.train(train_data_tf)\n","\n","    # Aggregate and update weights on the parameter server\n","    aggregated_weights = np.mean([worker.model.coefficients for worker in workers], axis=0)\n","    parameter_server.update_weights(aggregated_weights)\n","\n","# Evaluate the final model on the test set\n","final_weights = parameter_server.get_weights()\n","test_results = LogisticRegressionModel(weights=DenseVector(final_weights)).transform(test_data)\n","evaluator = BinaryClassificationEvaluator()\n","accuracy = evaluator.evaluate(test_results)\n","print(f\"Final accuracy on test set: {accuracy}\")\n","\n","# Stop the Spark session\n","spark.stop()\n"],"metadata":{"id":"-2xghC_MUkP0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##c.\tTensorFlow Architecture"],"metadata":{"id":"diQlI5_EVNRE"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.metrics import accuracy_score\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","# Generate synthetic data\n","X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_clusters_per_class=2, random_state=42)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Define the Parameter Server Model\n","class ParameterServerModel(tf.Module):\n","    def __init__(self, input_size):\n","        self.model = Sequential([\n","            Dense(1, input_shape=(input_size,), activation='sigmoid')\n","        ])\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Initialize the parameter server model\n","input_size = X_train.shape[1]\n","parameter_server_model = ParameterServerModel(input_size)\n","\n","# Define the training function for the workers\n","def train_worker(model, X, y, epochs=1):\n","    model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n","    model.fit(X, y, epochs=epochs, verbose=0)\n","\n","# Training loop\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    # Create a new model for each worker (assuming independent models)\n","    worker_model = SGDClassifier(loss='log')\n","\n","    # Train each worker on its subset of data\n","    train_worker(worker_model, X_train, y_train)\n","\n","    # Get the weights from each worker and update the parameter server model\n","    worker_weights = worker_model.coef_.flatten()\n","    parameter_server_model.model.layers[0].set_weights([worker_weights.reshape((input_size,)), np.array([0.0])])\n","\n","# Evaluate the final model on the test set\n","y_pred = np.round(parameter_server_model.forward(X_test).numpy()).flatten()\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Final accuracy on test set: {accuracy}\")\n"],"metadata":{"id":"raIkIVfrVRoO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#2.\t Execution of ML (or Big Data) Algorithms on parallel / distributed systems:\n","##a.\tPerformance Improvement and Trade-offs"],"metadata":{"id":"R13-q8qCV0im"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.metrics import accuracy_score\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.callbacks import EarlyStopping\n","import time\n","\n","# Generate synthetic data\n","X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_clusters_per_class=2, random_state=42)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize features for scikit-learn model\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Baseline SGDClassifier\n","baseline_model = SGDClassifier(loss='log', random_state=42)\n","baseline_model.fit(X_train_scaled, y_train)\n","y_pred_baseline = baseline_model.predict(X_test_scaled)\n","accuracy_baseline = accuracy_score(y_test, y_pred_baseline)\n","print(f\"Baseline SGDClassifier Accuracy: {accuracy_baseline}\")\n","\n","# TensorFlow Neural Network with Early Stopping\n","start_time = time.time()\n","\n","tf_model = Sequential([\n","    Dense(64, input_shape=(X_train_scaled.shape[1],), activation='relu'),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","tf_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Introduce early stopping to prevent overfitting\n","early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n","\n","# Train the model\n","history = tf_model.fit(\n","    X_train_scaled, y_train,\n","    epochs=50,\n","    batch_size=32,\n","    validation_split=0.2,\n","    callbacks=[early_stopping],\n","    verbose=0\n",")\n","\n","# Evaluate the model\n","y_pred_tf = np.round(tf_model.predict(X_test_scaled)).flatten()\n","accuracy_tf = accuracy_score(y_test, y_pred_tf)\n","print(f\"TensorFlow Neural Network Accuracy: {accuracy_tf}\")\n","\n","# Calculate training time\n","training_time = time.time() - start_time\n","print(f\"Training time: {training_time:.2f} seconds\")\n","\n","# Compare performance and trade-offs\n","print(\"\\nPerformance Comparison:\")\n","print(f\"Baseline SGDClassifier Accuracy: {accuracy_baseline}\")\n","print(f\"TensorFlow Neural Network Accuracy: {accuracy_tf}\")\n","print(f\"Training time: {training_time:.2f} seconds\")\n"],"metadata":{"id":"J_SbGncOWYAi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Distributed Neural Networks\n","##1.\tDecentralized and Local SGD – System Support (All-reduce, Asynchronous Parallelism)"],"metadata":{"id":"2GeA1csYl43D"}},{"cell_type":"code","source":["import tensorflow as tf\n","import horovod.tensorflow as hvd\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Initialize Horovod\n","hvd.init()\n","\n","# Pin GPU to be used to process local rank (one GPU per process)\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","if gpus:\n","    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n","\n","# Generate synthetic data\n","X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_clusters_per_class=2, random_state=42)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize features for scikit-learn model\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Define the SGDClassifier model (local model)\n","local_model = SGDClassifier(loss='log', random_state=42)\n","\n","# Wrap the local model with Horovod DistributedOptimizer\n","optimizer = hvd.DistributedOptimizer(tf.keras.optimizers.SGD(learning_rate=0.01))\n","local_model = tf.keras.estimator.model_to_estimator(local_model, optimizer=optimizer)\n","\n","# Train the local model\n","local_model.train(input_fn=lambda: input_fn(X_train_scaled, y_train), steps=100)\n","\n","# Synchronize the local models using Horovod's allreduce\n","hvd.allreduce(tf.constant(0, dtype=tf.float32))\n","\n","# Evaluate the synchronized model on the test set\n","y_pred = list(local_model.predict(input_fn=lambda: input_fn(X_test_scaled)))\n","y_pred = [pred['class_ids'][0] for pred in y_pred]\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Decentralized and Local SGD Accuracy: {accuracy}\")\n"],"metadata":{"id":"sjXxje9MmCme"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##2.\tLarge Scale Deep NN"],"metadata":{"id":"stqb_3rJm3Su"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers, models\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","\n","# Generate synthetic data\n","X, y = make_classification(n_samples=10000, n_features=50, n_informative=25, n_clusters_per_class=2, random_state=42)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize features for scikit-learn model\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Convert data to TensorFlow Dataset\n","train_dataset = tf.data.Dataset.from_tensor_slices((X_train_scaled, y_train)).shuffle(10000).batch(64)\n","test_dataset = tf.data.Dataset.from_tensor_slices((X_test_scaled, y_test)).batch(64)\n","\n","# Define a simple deep neural network model using Keras\n","def create_model():\n","    model = models.Sequential([\n","        layers.Dense(256, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n","        layers.Dense(128, activation='relu'),\n","        layers.Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","# Wrap the model with the distributed strategy\n","strategy = tf.distribute.MirroredStrategy()\n","with strategy.scope():\n","    distributed_model = create_model()\n","\n","# Train the model\n","distributed_model.fit(train_dataset, epochs=10)\n","\n","# Evaluate the model on the test set\n","y_pred = distributed_model.predict(test_dataset)\n","y_pred_binary = (y_pred > 0.5).astype(int)\n","accuracy = accuracy_score(y_test, y_pred_binary)\n","print(f\"Accuracy on test set: {accuracy}\")\n"],"metadata":{"id":"fPo0QtuWm8wh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##3.\tSystems for Federated Learning"],"metadata":{"id":"U2JzTDbAnP5r"}},{"cell_type":"code","source":["import tensorflow as tf\n","import tensorflow_federated as tff\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","\n","# Generate synthetic data\n","X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_clusters_per_class=2, random_state=42)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize features for scikit-learn model\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Convert data to TensorFlow Dataset\n","train_dataset = tf.data.Dataset.from_tensor_slices((X_train_scaled, y_train)).shuffle(100).batch(10)\n","test_dataset = tf.data.Dataset.from_tensor_slices((X_test_scaled, y_test)).batch(10)\n","\n","# Define a simple Keras model\n","def create_keras_model():\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n","        tf.keras.layers.Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","# Wrap the Keras model with TFF\n","def create_tff_model():\n","    keras_model = create_keras_model()\n","    return tff.learning.from_keras_model(\n","        keras_model,\n","        input_spec=train_dataset.element_spec,\n","        loss=tf.keras.losses.BinaryCrossentropy(),\n","        metrics=[tf.keras.metrics.BinaryAccuracy()])\n","\n","# Create a Federated Averaging process\n","def create_federated_averaging_process(model):\n","    return tff.learning.build_federated_averaging_process(\n","        model,\n","        client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n","        server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))\n","\n","# Simulate Federated Learning\n","federated_train_data = [train_dataset]  # For simplicity, using the same data on the client\n","tff_model = create_tff_model()\n","federated_averaging_process = create_federated_averaging_process(tff_model)\n","state = federated_averaging_process.initialize()\n","\n","NUM_ROUNDS = 10\n","for round_num in range(NUM_ROUNDS):\n","    state, metrics = federated_averaging_process.next(state, federated_train_data)\n","    print(f'Round {round_num}: {metrics}')\n","\n","# Evaluate the federated model on the test set\n","def evaluate_federated_model(state, test_dataset):\n","    evaluation = tff.learning.build_federated_evaluation(tff_model)\n","    return evaluation(state.model, [test_dataset])\n","\n","test_metrics = evaluate_federated_model(state, test_dataset)\n","print(f'Test Metrics: {test_metrics}')\n"],"metadata":{"id":"ko43bq8WnWPW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**M4**\n","##ML Deployment on Constrained Systems I:\n","##1.\tModel Compression, Compression vs. Inference"],"metadata":{"id":"bsXaaBljnwV8"}},{"cell_type":"code","source":["# Install necessary packages\n","# pip install scikit-learn tensorflow\n","\n","# Import libraries\n","from sklearn.datasets import load_digits\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn import datasets\n","from sklearn.decomposition import TruncatedSVD\n","import tensorflow as tf\n","import time\n","\n","# Load dataset\n","digits = load_digits()\n","X, y = digits.data, digits.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train a RandomForestClassifier (original model)\n","original_model = RandomForestClassifier(n_estimators=100, random_state=42)\n","original_model.fit(X_train, y_train)\n","\n","# Evaluate original model\n","original_predictions = original_model.predict(X_test)\n","original_accuracy = accuracy_score(y_test, original_predictions)\n","print(f\"Original Model Accuracy: {original_accuracy:.4f}\")\n","\n","# Compress the model using TruncatedSVD\n","n_components = 20\n","svd = TruncatedSVD(n_components=n_components)\n","X_train_compressed = svd.fit_transform(X_train)\n","X_test_compressed = svd.transform(X_test)\n","\n","# Train a RandomForestClassifier on the compressed data\n","compressed_model = RandomForestClassifier(n_estimators=100, random_state=42)\n","compressed_model.fit(X_train_compressed, y_train)\n","\n","# Evaluate compressed model\n","compressed_predictions = compressed_model.predict(X_test_compressed)\n","compressed_accuracy = accuracy_score(y_test, compressed_predictions)\n","print(f\"Compressed Model Accuracy: {compressed_accuracy:.4f}\")\n","\n","# TensorFlow for inference\n","# Convert the compressed model to TensorFlow Lite format\n","converter = tf.lite.TFLiteConverter.from_scikit_learn(model=compressed_model)\n","tflite_model = converter.convert()\n","\n","# Save the TensorFlow Lite model to a file\n","with open('compressed_model.tflite', 'wb') as f:\n","    f.write(tflite_model)\n","\n","# Load the TensorFlow Lite model and allocate tensors.\n","interpreter = tf.lite.Interpreter(model_content=tflite_model)\n","interpreter.allocate_tensors()\n","\n","# Run inference on a single example\n","input_tensor_index = interpreter.get_input_details()[0]['index']\n","output = interpreter.tensor(interpreter.get_output_details()[0]['index'])\n","input_example = X_test_compressed[0].reshape(1, -1).astype('float32')\n","interpreter.set_tensor(input_tensor_index, input_example)\n","start_time = time.time()\n","interpreter.invoke()\n","inference_time = time.time() - start_time\n","\n","# Get the output and print the result\n","inference_result = output()[0]\n","print(f\"\\nInference Result: {inference_result}\")\n","print(f\"Inference Time: {inference_time:.4f} seconds\")\n"],"metadata":{"id":"B2ztHOVZoH9e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##2.\tQuantization and Learning with Limited Numerical Precision"],"metadata":{"id":"i5v15uLVoqYO"}},{"cell_type":"code","source":["# Install necessary packages\n","# pip install scikit-learn tensorflow\n","\n","# Import libraries\n","from sklearn.datasets import load_digits\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","import tensorflow as tf\n","import time\n","\n","# Load dataset\n","digits = load_digits()\n","X, y = digits.data, digits.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train a RandomForestClassifier (original model)\n","original_model = RandomForestClassifier(n_estimators=100, random_state=42)\n","original_model.fit(X_train, y_train)\n","\n","# Evaluate original model\n","original_predictions = original_model.predict(X_test)\n","original_accuracy = accuracy_score(y_test, original_predictions)\n","print(f\"Original Model Accuracy: {original_accuracy:.4f}\")\n","\n","# TensorFlow for quantization\n","converter = tf.lite.TFLiteConverter.from_scikit_learn(model=original_model)\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","quantized_tflite_model = converter.convert()\n","\n","# Save the quantized TensorFlow Lite model to a file\n","with open('quantized_model.tflite', 'wb') as f:\n","    f.write(quantized_tflite_model)\n","\n","# Load the TensorFlow Lite model and allocate tensors.\n","interpreter_quantized = tf.lite.Interpreter(model_content=quantized_tflite_model)\n","interpreter_quantized.allocate_tensors()\n","\n","# Run inference on a single example\n","input_tensor_index_quantized = interpreter_quantized.get_input_details()[0]['index']\n","output_quantized = interpreter_quantized.tensor(interpreter_quantized.get_output_details()[0]['index'])\n","input_example_quantized = X_test[0].reshape(1, -1).astype('float32')\n","interpreter_quantized.set_tensor(input_tensor_index_quantized, input_example_quantized)\n","start_time_quantized = time.time()\n","interpreter_quantized.invoke()\n","inference_time_quantized = time.time() - start_time_quantized\n","\n","# Get the output and print the result\n","inference_result_quantized = output_quantized()[0]\n","print(f\"\\nQuantized Inference Result: {inference_result_quantized}\")\n","print(f\"Quantized Inference Time: {inference_time_quantized:.4f} seconds\")\n"],"metadata":{"id":"Ns_zvuaWoy6i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Neural Network Pruning\n","##1.\t Pruning of CNNs"],"metadata":{"id":"uBRkG4BIpJ4X"}},{"cell_type":"code","source":["# Install necessary packages\n","# pip install scikit-learn tensorflow\n","\n","# Import libraries\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_digits\n","from sklearn.neural_network import MLPClassifier\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow_model_optimization.sparsity import keras as sparsity\n","\n","# Load the digits dataset\n","digits = load_digits()\n","X, y = digits.data, digits.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Preprocess the data\n","X_train = X_train.reshape(X_train.shape[0], 8, 8, 1).astype('float32') / 16.0\n","X_test = X_test.reshape(X_test.shape[0], 8, 8, 1).astype('float32') / 16.0\n","\n","# Define and train a simple CNN model using Keras\n","model = Sequential([\n","    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(8, 8, 1)),\n","    layers.MaxPooling2D((2, 2)),\n","    layers.Flatten(),\n","    layers.Dense(128, activation='relu'),\n","    layers.Dense(10, activation='softmax')\n","])\n","\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n","model.fit(X_train, y_train, epochs=20, validation_split=0.2, callbacks=[early_stopping])\n","\n","# Evaluate the original model\n","original_predictions = np.argmax(model.predict(X_test), axis=1)\n","original_accuracy = accuracy_score(y_test, original_predictions)\n","print(f\"Original Model Accuracy: {original_accuracy:.4f}\")\n","\n","# Apply pruning to the model\n","pruning_params = {\n","    'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.50, final_sparsity=0.80, begin_step=0, end_step=2000)\n","}\n","\n","pruned_model = sparsity.prune_low_magnitude(model, **pruning_params)\n","\n","# Compile the pruned model\n","pruned_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the pruned model\n","pruned_model.fit(X_train, y_train, epochs=20, validation_split=0.2, callbacks=[early_stopping])\n","\n","# Evaluate the pruned model\n","pruned_predictions = np.argmax(pruned_model.predict(X_test), axis=1)\n","pruned_accuracy = accuracy_score(y_test, pruned_predictions)\n","print(f\"Pruned Model Accuracy: {pruned_accuracy:.4f}\")\n","\n","# Save the pruned model\n","pruned_model.save('pruned_model.h5')\n"],"metadata":{"id":"Zfru0lOvpVlH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Evaluation of Pruning"],"metadata":{"id":"LZbIBCd0plt4"}},{"cell_type":"code","source":["# Install necessary packages\n","# pip install scikit-learn tensorflow\n","\n","# Import libraries\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.datasets import load_digits\n","from sklearn.neural_network import MLPClassifier\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import load_model\n","from tensorflow_model_optimization.sparsity import keras as sparsity\n","\n","# Load the digits dataset\n","digits = load_digits()\n","X, y = digits.data, digits.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Preprocess the data\n","X_train = X_train.reshape(X_train.shape[0], 8, 8, 1).astype('float32') / 16.0\n","X_test = X_test.reshape(X_test.shape[0], 8, 8, 1).astype('float32') / 16.0\n","\n","# Load the original model\n","original_model = load_model('original_model.h5')\n","\n","# Evaluate the original model\n","original_predictions = np.argmax(original_model.predict(X_test), axis=1)\n","original_accuracy = accuracy_score(y_test, original_predictions)\n","print(f\"Original Model Accuracy: {original_accuracy:.4f}\")\n","print(\"Classification Report for Original Model:\")\n","print(classification_report(y_test, original_predictions))\n","\n","# Load the pruned model\n","pruned_model = load_model('pruned_model.h5')\n","\n","# Evaluate the pruned model\n","pruned_predictions = np.argmax(pruned_model.predict(X_test), axis=1)\n","pruned_accuracy = accuracy_score(y_test, pruned_predictions)\n","print(f\"\\nPruned Model Accuracy: {pruned_accuracy:.4f}\")\n","print(\"Classification Report for Pruned Model:\")\n","print(classification_report(y_test, pruned_predictions))\n","\n","# Compare the sparsity of the pruned model\n","pruned_params = pruned_model.get_config()['layers'][1]['config']['kernel_regularizer']['config']\n","final_sparsity = pruned_params['final_sparsity']\n","print(f\"\\nFinal Sparsity of Pruned Model: {final_sparsity:.4f}\")\n"],"metadata":{"id":"d06loNE1pr5o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##3.\t Deep Compression: Leveraging quantization, pruning, and sparsity."],"metadata":{"id":"fjA2nwTtp_ow"}},{"cell_type":"code","source":["# Install necessary packages\n","# pip install scikit-learn tensorflow\n","\n","# Import libraries\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.datasets import load_digits\n","from sklearn.neural_network import MLPClassifier\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow_model_optimization.sparsity import keras as sparsity\n","\n","# Load the digits dataset\n","digits = load_digits()\n","X, y = digits.data, digits.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Preprocess the data\n","X_train = X_train.reshape(X_train.shape[0], 8, 8, 1).astype('float32') / 16.0\n","X_test = X_test.reshape(X_test.shape[0], 8, 8, 1).astype('float32') / 16.0\n","\n","# Define and train a simple CNN model using Keras\n","model = Sequential([\n","    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(8, 8, 1)),\n","    layers.MaxPooling2D((2, 2)),\n","    layers.Flatten(),\n","    layers.Dense(128, activation='relu'),\n","    layers.Dense(10, activation='softmax')\n","])\n","\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the original model\n","model.fit(X_train, y_train, epochs=10, validation_split=0.2)\n","\n","# Evaluate the original model\n","original_predictions = np.argmax(model.predict(X_test), axis=1)\n","original_accuracy = accuracy_score(y_test, original_predictions)\n","print(f\"Original Model Accuracy: {original_accuracy:.4f}\")\n","print(\"Classification Report for Original Model:\")\n","print(classification_report(y_test, original_predictions))\n","\n","# Quantization of the model\n","quantize_model = sparsity.prune_low_magnitude(model)\n","\n","# Compile the quantized model\n","quantize_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the quantized model\n","quantize_model.fit(X_train, y_train, epochs=10, validation_split=0.2)\n","\n","# Evaluate the quantized model\n","quantized_predictions = np.argmax(quantize_model.predict(X_test), axis=1)\n","quantized_accuracy = accuracy_score(y_test, quantized_predictions)\n","print(f\"\\nQuantized Model Accuracy: {quantized_accuracy:.4f}\")\n","print(\"Classification Report for Quantized Model:\")\n","print(classification_report(y_test, quantized_predictions))\n","\n","# Save the quantized model\n","quantize_model.save('quantized_model.h5')\n"],"metadata":{"id":"p480tQ7UqPWQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#ML Deployment on Constrained Systems II:\n","##1.\tTinyML and TensorFlow Lite;"],"metadata":{"id":"dNrRaY0jqn0Y"}},{"cell_type":"code","source":["# Install necessary packages\n","# pip install scikit-learn tensorflow\n","\n","# Import libraries\n","import numpy as np\n","from sklearn.datasets import load_digits\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.ensemble import RandomForestClassifier\n","import tensorflow as tf\n","from tensorflow import lite\n","\n","# Load the digits dataset\n","digits = load_digits()\n","X, y = digits.data, digits.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train a RandomForestClassifier (original model)\n","original_model = RandomForestClassifier(n_estimators=100, random_state=42)\n","original_model.fit(X_train, y_train)\n","\n","# Evaluate original model\n","original_predictions = original_model.predict(X_test)\n","original_accuracy = accuracy_score(y_test, original_predictions)\n","print(f\"Original Model Accuracy: {original_accuracy:.4f}\")\n","\n","# Convert the model to TensorFlow Lite format\n","converter = lite.TFLiteConverter.from_scikit_learn(model=original_model)\n","tflite_model = converter.convert()\n","\n","# Save the TensorFlow Lite model to a file\n","with open('model.tflite', 'wb') as f:\n","    f.write(tflite_model)\n","\n","# Load the TensorFlow Lite model and allocate tensors.\n","interpreter = lite.Interpreter(model_content=tflite_model)\n","interpreter.allocate_tensors()\n","\n","# Run inference on a single example\n","input_tensor_index = interpreter.get_input_details()[0]['index']\n","output = interpreter.tensor(interpreter.get_output_details()[0]['index'])\n","input_example = X_test[0].reshape(1, -1).astype('float32')\n","interpreter.set_tensor(input_tensor_index, input_example)\n","interpreter.invoke()\n","\n","# Get the output and print the result\n","inference_result = output()[0]\n","predicted_class = np.argmax(inference_result)\n","print(f\"\\nInference Result: {inference_result}\")\n","print(f\"Predicted Class: {predicted_class}\")\n"],"metadata":{"id":"lH-SHRdsq2je"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##2.\tEnergy Constraints – Adapting Algorithms for Constrained Devices;"],"metadata":{"id":"dNnpb0eyrIRz"}},{"cell_type":"code","source":["# Install necessary packages\n","# pip install scikit-learn tensorflow\n","\n","# Import libraries\n","import numpy as np\n","from sklearn.datasets import load_digits\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","import tensorflow as tf\n","from tensorflow import lite\n","\n","# Load the digits dataset\n","digits = load_digits()\n","X, y = digits.data, digits.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train a RandomForestClassifier (original model)\n","original_model = RandomForestClassifier(n_estimators=100, random_state=42)\n","original_model.fit(X_train, y_train)\n","\n","# Evaluate original model\n","original_predictions = original_model.predict(X_test)\n","original_accuracy = accuracy_score(y_test, original_predictions)\n","print(f\"Original Model Accuracy: {original_accuracy:.4f}\")\n","\n","# Convert the model to TensorFlow Lite format with quantization\n","converter = lite.TFLiteConverter.from_scikit_learn(model=original_model)\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","tflite_model = converter.convert()\n","\n","# Save the TensorFlow Lite model to a file\n","with open('energy_constrained_model.tflite', 'wb') as f:\n","    f.write(tflite_model)\n","\n","# Load the TensorFlow Lite model and allocate tensors.\n","interpreter = lite.Interpreter(model_content=tflite_model)\n","interpreter.allocate_tensors()\n","\n","# Run inference on a single example\n","input_tensor_index = interpreter.get_input_details()[0]['index']\n","output = interpreter.tensor(interpreter.get_output_details()[0]['index'])\n","input_example = X_test[0].reshape(1, -1).astype('float32')\n","interpreter.set_tensor(input_tensor_index, input_example)\n","interpreter.invoke()\n","\n","# Get the output and print the result\n","inference_result = output()[0]\n","predicted_class = np.argmax(inference_result)\n","print(f\"\\nInference Result: {inference_result}\")\n","print(f\"Predicted Class: {predicted_class}\")\n"],"metadata":{"id":"S0zhvy9irQMa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##3.\tAssessing the tradeoffs - Accuracy of prediction, Model Size, Throughput, Response Time, Energy Consumption\n","\n","\n","\n"],"metadata":{"id":"Gk6JEFkernFe"}},{"cell_type":"code","source":["# Install necessary packages\n","# pip install scikit-learn tensorflow\n","\n","# Import libraries\n","import numpy as np\n","from sklearn.datasets import load_digits\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","import tensorflow as tf\n","from tensorflow import lite\n","import time\n","\n","# Load the digits dataset\n","digits = load_digits()\n","X, y = digits.data, digits.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train a RandomForestClassifier (original model)\n","original_model = RandomForestClassifier(n_estimators=100, random_state=42)\n","original_model.fit(X_train, y_train)\n","\n","# Evaluate original model\n","original_predictions = original_model.predict(X_test)\n","original_accuracy = accuracy_score(y_test, original_predictions)\n","print(f\"Original Model Accuracy: {original_accuracy:.4f}\")\n","\n","# Convert the model to TensorFlow Lite format with quantization\n","converter = lite.TFLiteConverter.from_scikit_learn(model=original_model)\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","tflite_model = converter.convert()\n","\n","# Save the TensorFlow Lite model to a file\n","with open('tradeoff_model.tflite', 'wb') as f:\n","    f.write(tflite_model)\n","\n","# Load the TensorFlow Lite model and allocate tensors.\n","interpreter = lite.Interpreter(model_content=tflite_model)\n","interpreter.allocate_tensors()\n","\n","# Measure inference time\n","num_samples = 100\n","inference_times = []\n","\n","for _ in range(num_samples):\n","    input_tensor_index = interpreter.get_input_details()[0]['index']\n","    output = interpreter.tensor(interpreter.get_output_details()[0]['index'])\n","    input_example = X_test[0].reshape(1, -1).astype('float32')\n","    interpreter.set_tensor(input_tensor_index, input_example)\n","\n","    start_time = time.time()\n","    interpreter.invoke()\n","    inference_time = time.time() - start_time\n","    inference_times.append(inference_time)\n","\n","# Calculate average inference time\n","average_inference_time = np.mean(inference_times)\n","print(f\"\\nAverage Inference Time: {average_inference_time:.4f} seconds\")\n","\n","# Assess tradeoffs\n","model_size = len(tflite_model) / (1024 * 1024)  # in megabytes\n","throughput = num_samples / average_inference_time  # predictions per second\n","response_time = average_inference_time  # seconds\n","energy_consumption = average_inference_time  # a simplified measure, considering energy per second\n","\n","print(f\"\\nTradeoff Assessment:\")\n","print(f\"Model Size: {model_size:.4f} MB\")\n","print(f\"Throughput: {throughput:.4f} predictions/second\")\n","print(f\"Response Time: {response_time:.4f} seconds\")\n","print(f\"Energy Consumption: {energy_consumption:.4f} J (assuming constant energy consumption)\")\n"],"metadata":{"id":"uZ-tWqgZru45"},"execution_count":null,"outputs":[]}]}